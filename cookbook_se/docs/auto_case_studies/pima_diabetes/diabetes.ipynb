{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Write an end to end ML pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import typing\nfrom collections import OrderedDict\n\nfrom dataclasses import dataclass\nfrom dataclasses_json import dataclass_json\nimport joblib\nimport pandas as pd\nfrom flytekit import task, workflow\nfrom flytekit.types.file import FlyteFile\nfrom flytekit.types.schema import FlyteSchema\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we are working with a specific dataset, we will create a strictly typed schema for the dataset.\nIf we wanted a generic data splitter we could use a Generic schema without any column type and name information\nExample file: https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\nCSV Columns\n\n#. Number of times pregnant\n#. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n#. Diastolic blood pressure (mm Hg)\n#. Triceps skin fold thickness (mm)\n#. 2-Hour serum insulin (mu U/ml)\n#. Body mass index (weight in kg/(height in m)^2)\n#. Diabetes pedigree function\n#. Age (years)\n#. Class variable (0 or 1)\n\nExample Row: 6,148,72,35,0,33.6,0.627,50,1\nthe input dataset schema\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "DATASET_COLUMNS = OrderedDict(\n    {\n        \"#preg\": int,\n        \"pgc_2h\": int,\n        \"diastolic_bp\": int,\n        \"tricep_skin_fold_mm\": int,\n        \"serum_insulin_2h\": int,\n        \"bmi\": float,\n        \"diabetes_pedigree\": float,\n        \"age\": int,\n        \"class\": int,\n    }\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first 8 columns are features\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "FEATURE_COLUMNS = OrderedDict(\n    {k: v for k, v in DATASET_COLUMNS.items() if k != \"class\"}\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last column is the class\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "CLASSES_COLUMNS = OrderedDict({\"class\": int})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us declare a task that accepts a CSV file with the previously defined\ncolumns and converts it to a typed schema.\nAn example CSV file is available at\n`https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv<https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv>`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@task(cache_version=\"1.0\", cache=True, memory_limit=\"200Mi\")\ndef split_traintest_dataset(\n    dataset: FlyteFile[typing.TypeVar(\"csv\")], seed: int, test_split_ratio: float\n) -> (\n    FlyteSchema[FEATURE_COLUMNS],\n    FlyteSchema[FEATURE_COLUMNS],\n    FlyteSchema[CLASSES_COLUMNS],\n    FlyteSchema[CLASSES_COLUMNS],\n):\n    \"\"\"\n    Retrieves the training dataset from the given blob location and then splits it using the split ratio and returns the result\n    This splitter is only for the dataset that has the format as specified in the example csv. The last column is assumed to be\n    the class and all other columns 0-8 the features.\n\n    The data is returned as a schema, which gets converted to a parquet file in the back.\n    \"\"\"\n    column_names = [k for k in DATASET_COLUMNS.keys()]\n    df = pd.read_csv(dataset, names=column_names)\n\n    # Select all features\n    x = df[column_names[:8]]\n    # Select only the classes\n    y = df[[column_names[-1]]]\n\n    # split data into train and test sets\n    return train_test_split(x, y, test_size=test_split_ratio, random_state=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is also possible to defined the output file type. This is useful in\ncombining tasks, where one task may only accept models serialized in ``.joblib.dat``\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "MODELSER_JOBLIB = typing.TypeVar(\"joblib.dat\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is also possible in Flyte to pass custom objects, as long as they are\ndeclared as ``dataclass``es and also decorated with ``@dataclass_json``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@dataclass_json\n@dataclass\nclass XGBoostModelHyperparams(object):\n    \"\"\"\n    These are the xgboost hyper parameters available in scikit-learn library.\n    \"\"\"\n\n    max_depth: int = 3\n    learning_rate: float = 0.1\n    n_estimators: int = 100\n    objective: str = \"binary:logistic\"\n    booster: str = \"gbtree\"\n    n_jobs: int = 1\n\n\nmodel_file = typing.NamedTuple(\"Model\", model=FlyteFile[MODELSER_JOBLIB])\nworkflow_outputs = typing.NamedTuple(\n    \"WorkflowOutputs\", model=FlyteFile[MODELSER_JOBLIB], accuracy=float\n)\n\n\n@task(cache_version=\"1.0\", cache=True, memory_limit=\"200Mi\")\ndef fit(\n    x: FlyteSchema[FEATURE_COLUMNS],\n    y: FlyteSchema[CLASSES_COLUMNS],\n    hyperparams: XGBoostModelHyperparams,\n) -> model_file:\n    \"\"\"\n    This function takes the given input features and their corresponding classes to train a XGBClassifier.\n    NOTE: We have simplified the number of hyper parameters we take for demo purposes\n    \"\"\"\n    x_df = x.open().all()\n    y_df = y.open().all()\n\n    # fit model no training data\n    m = XGBClassifier(\n        n_jobs=hyperparams.n_jobs,\n        max_depth=hyperparams.max_depth,\n        n_estimators=hyperparams.n_estimators,\n        booster=hyperparams.booster,\n        objective=hyperparams.objective,\n        learning_rate=hyperparams.learning_rate,\n    )\n    m.fit(x_df, y_df)\n\n    # TODO model Blob should be a file like object\n    fname = \"model.joblib.dat\"\n    joblib.dump(m, fname)\n    return (fname,)\n\n\n@task(cache_version=\"1.0\", cache=True, memory_limit=\"200Mi\")\ndef predict(\n    x: FlyteSchema[FEATURE_COLUMNS], model_ser: FlyteFile[MODELSER_JOBLIB],\n) -> FlyteSchema[CLASSES_COLUMNS]:\n    \"\"\"\n    Given a any trained model, serialized using joblib (this method can be shared!) and features, this method returns\n    predictions.\n    \"\"\"\n    model = joblib.load(model_ser)\n    # make predictions for test data\n    x_df = x.open().all()\n    y_pred = model.predict(x_df)\n\n    col = [k for k in CLASSES_COLUMNS.keys()]\n    y_pred_df = pd.DataFrame(y_pred, columns=col, dtype=\"int64\")\n    y_pred_df.round(0)\n    return y_pred_df\n\n\n@task(cache_version=\"1.0\", cache=True, memory_limit=\"200Mi\")\ndef score(\n    predictions: FlyteSchema[CLASSES_COLUMNS], y: FlyteSchema[CLASSES_COLUMNS]\n) -> float:\n    \"\"\"\n    Compares the predictions with the actuals and returns the accuracy score.\n    \"\"\"\n    pred_df = predictions.open().all()\n    y_df = y.open().all()\n    # evaluate predictions\n    acc = accuracy_score(y_df, pred_df)\n    print(\"Accuracy: %.2f%%\" % (acc * 100.0))\n    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Workflow sample here\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@workflow\ndef diabetes_xgboost_model(\n    dataset: FlyteFile[\n        typing.TypeVar(\"csv\")\n    ] = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\",\n    test_split_ratio: float = 0.33,\n    seed: int = 7,\n) -> workflow_outputs:\n    \"\"\"\n    This pipeline trains an XGBoost mode for any given dataset that matches the schema as specified in\n    https://github.com/jbrownlee/Datasets/blob/master/pima-indians-diabetes.names.\n    \"\"\"\n    x_train, x_test, y_train, y_test = split_traintest_dataset(\n        dataset=dataset, seed=seed, test_split_ratio=test_split_ratio\n    )\n    model = fit(x=x_train, y=y_train, hyperparams=XGBoostModelHyperparams(max_depth=4),)\n    predictions = predict(x=x_test, model_ser=model.model)\n    return model.model, score(predictions=predictions, y=y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The entire workflow can be executed locally as follows...\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    print(f\"Running {__file__} main...\")\n    print(diabetes_xgboost_model())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}