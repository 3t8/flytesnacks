{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Custom training algorithms on Amazon Sagemaker\nThis script shows an example of how you can simply convert your tensorflow training scripts to run on Amazon Sagemaker\nwith very few modifications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import typing\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom flytekit import task, workflow\nfrom flytekitplugins.awssagemaker import (\n    SagemakerTrainingJobConfig,\n    TrainingJobResourceConfig,\n    AlgorithmSpecification,\n    InputMode,\n    AlgorithmName,\n    InputContentType,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Algorithm\nIn this custom algorithm we will train MNIST using tensorflow.\nThe Training will produce 2 outputs\n\n#. The serialized model in HDF5 format\n#. And a log dictionary which is the Keras - `History.history`. This contains the accuracies and loss values\n#. Tensorboard Logs: We will also output A Directory that contains Tensorboard compatible logs. Flyte will collect\n   these logs and make them available for visualization in tensorboard - locally or if running remote.\n\nRefer to section `sagemaker_tensorboard` to visualize the outputs of this example.\n\n\n## [Optional]: Create specialized type aliases for files in specific formats\nThe trained model will be serialized using HDF5 encoding. We can create a type for such a file. This type is simply\nan informative way of understanding what type of file is generated and later on helps in matching up tasks that can\nwork on similar files. This completely optional.\n\n.. code-block:: python\n\n      HDF5EncodedModelFile = FlyteFile[typing.TypeVar(\"hdf5\")]\n\nBut this type alias is already available from Flytekit's type engine, so we can just import it.\nWe will also import ``PNGImageFile`` which we will use in the next task\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from flytekit.types.file import HDF5EncodedFile, PNGImageFile\nfrom flytekit.types.directory import TensorboardLogs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can create a named tuple to name the specific outputs from the training. This is optional as well, but\nis useful to have human readable names for the outputs. In the absence of this, flytekit will name all outputs as\n``o0, o1, ...``\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "TrainingOutputs = typing.NamedTuple(\n    \"TrainingOutputs\", model=HDF5EncodedFile, epoch_logs=dict, logs=TensorboardLogs\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Actual Algorithm\nTo ensure that the code runs on Sagemaker, create a sagemaker task config using the class\n``SagemakerTrainingJobConfig``\n\n .. code::python\n\n      @task(\n       task_config=SagemakerTrainingJobConfig(\n        algorithm_specification=...,\n        training_job_resource_config=...,\n       )\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def normalize_img(image, label):\n    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n    return tf.cast(image, tf.float32) / 255.0, label\n\n\n@task(\n    task_config=SagemakerTrainingJobConfig(\n        algorithm_specification=AlgorithmSpecification(\n            input_mode=InputMode.FILE,\n            algorithm_name=AlgorithmName.CUSTOM,\n            algorithm_version=\"\",\n            input_content_type=InputContentType.TEXT_CSV,\n        ),\n        training_job_resource_config=TrainingJobResourceConfig(\n            instance_type=\"ml.m4.xlarge\", instance_count=1, volume_size_in_gb=25,\n        ),\n    ),\n    cache_version=\"1.0\",\n    cache=True,\n    container_image=\"{{.image.sagemaker.fqn}}:sagemaker-{{.image.default.version}}\",\n)\ndef custom_training_task(epochs: int, batch_size: int) -> TrainingOutputs:\n    (ds_train, ds_test), ds_info = tfds.load(\n        \"mnist\",\n        split=[\"train\", \"test\"],\n        shuffle_files=True,\n        as_supervised=True,\n        with_info=True,\n    )\n\n    ds_train = ds_train.map(\n        normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE\n    )\n    ds_train = ds_train.cache()\n    ds_train = ds_train.shuffle(ds_info.splits[\"train\"].num_examples)\n    ds_train = ds_train.batch(batch_size)\n    ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n\n    ds_test = ds_test.map(\n        normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE\n    )\n    ds_test = ds_test.batch(batch_size)\n    ds_test = ds_test.cache()\n    ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n\n    model = tf.keras.models.Sequential(\n        [\n            tf.keras.layers.Flatten(input_shape=(28, 28)),\n            tf.keras.layers.Dense(128, activation=\"relu\"),\n            tf.keras.layers.Dense(10),\n        ]\n    )\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(0.001),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n    )\n\n    log_dir = \"/tmp/training-logs\"\n    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n\n    history = model.fit(\n        ds_train, epochs=epochs, validation_data=ds_test, callbacks=[tb_callback],\n    )\n\n    serialized_model = \"my_model.h5\"\n    model.save(serialized_model, overwrite=True)\n\n    return TrainingOutputs(\n        model=HDF5EncodedFile(serialized_model),\n        epoch_logs=history.history,\n        logs=TensorboardLogs(log_dir),\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the metrics\nIn the following task we will use the history logs from the training in the previous step and plot the curves using\nmatplotlib. Images will be output as png.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "PlotOutputs = typing.NamedTuple(\"PlotOutputs\", accuracy=PNGImageFile, loss=PNGImageFile)\n\n\n@task\ndef plot_loss_and_accuracy(epoch_logs: dict) -> PlotOutputs:\n    # summarize history for accuracy\n    plt.plot(epoch_logs[\"sparse_categorical_accuracy\"])\n    plt.plot(epoch_logs[\"val_sparse_categorical_accuracy\"])\n    plt.title(\"Sparse Categorical accuracy\")\n    plt.ylabel(\"accuracy\")\n    plt.xlabel(\"epoch\")\n    plt.legend([\"train\", \"test\"], loc=\"upper left\")\n    accuracy_plot = \"accuracy.png\"\n    plt.savefig(accuracy_plot)\n    # summarize history for loss\n    plt.plot(epoch_logs[\"loss\"])\n    plt.plot(epoch_logs[\"val_loss\"])\n    plt.title(\"model loss\")\n    plt.ylabel(\"loss\")\n    plt.xlabel(\"epoch\")\n    plt.legend([\"train\", \"test\"], loc=\"upper left\")\n    loss_plot = \"loss.png\"\n    plt.savefig(loss_plot)\n\n    return PlotOutputs(accuracy=PNGImageFile(accuracy_plot), loss=PNGImageFile(loss_plot))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The workflow takes in the hyperparams - in this case just the epochs and the batch_size and outputs the trained model\nand the plotted curves\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@workflow\ndef mnist_trainer(\n    epochs: int = 5, batch_size: int = 128\n) -> (HDF5EncodedFile, PNGImageFile, PNGImageFile, TensorboardLogs):\n    model, history, logs = custom_training_task(epochs=epochs, batch_size=batch_size)\n    accuracy, loss = plot_loss_and_accuracy(epoch_logs=history)\n    return model, accuracy, loss, logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As long as you have tensorflow setup locally, it will run like a regular python script\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    model, accurracy, loss, logs = mnist_trainer()\n    print(\n        f\"Model: {model}, Accuracy PNG: {accurracy}, loss PNG: {loss}, Tensorboard Log Dir: {logs}\"\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Rendering the output logs in tensorboard\nWhen running locally, the output of execution looks like\n\n.. code-block::\n\n  Model: /tmp/flyte/20210110_214129/mock_remote/8421ae4d041f76488e245edf3f4360d5/my_model.h5, Accuracy PNG: /tmp/flyte/20210110_214129/mock_remote/cf6a2cd9d3ded89ed814278a8fb3678c/accuracy.png, loss PNG: /tmp/flyte/20210110_214129/mock_remote/267c9dd17d4d4e7c9c8bb8b12ef1e3d2/loss.png, Tensorboard Log Dir: /tmp/flyte/20210110_214129/mock_remote/a4b04e58e21f26f08f81df24094d6446/\n\nYou can use the ``Tensorboard Log Dir: /tmp/flyte/20210110_214129/mock_remote/a4b04e58e21f26f08f81df24094d6446/`` as\nan input to tensorboard to visualize the training as follows\n\n.. prompt:: bash\n\n  tensorboard --logdir /tmp/flyte/20210110_214129/mock_remote/a4b04e58e21f26f08f81df24094d6446/\n\n\nIf running remotely (executing on Flyte hosted environment), the workflow execution outputs can be retrieved.\nRefer to .. TODO.\nYou can retrieve the outputs - which will be a path to a blob store like S3, GCS, minio, etc. Tensorboad can be\npointed to on your local laptop to visualize the results.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}