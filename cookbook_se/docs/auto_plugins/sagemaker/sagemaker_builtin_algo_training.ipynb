{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Training builtin algorithms on Amazon Sagemaker\nIn this example we will show how it is possible to work with Built-in algorithms with Amazon sagemaker and even\nperform Hyper parameter optimization using Sagemaker HPO\n\n\n## Defining a XGBoost Training job\nWe will create a job that will train an XGBoost model using the prebuilt algorithms @Sagemaker.\nRefer to `Sagemaker XGBoost docs here <https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html>`_\nTo understand more about XGBoost refer `here <https://xgboost.readthedocs.io/en/latest/>`_.\nTo dive deeper into the Flytekit API refer to\n`docs <https://lyft.github.io/flyte/flytekit/flytekit.common.tasks.sagemaker.html?highlight=sagemaker#module-flytekit.common.tasks.sagemaker>`_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import typing\n\nfrom flytekit import TaskMetadata\nfrom flytekitplugins.awssagemaker import (\n    SagemakerBuiltinAlgorithmsTask,\n    SagemakerTrainingJobConfig,\n    SagemakerHPOTask,\n    HPOJob,\n    HyperparameterTuningJobConfig,\n    HyperparameterTuningObjectiveType,\n    HyperparameterTuningStrategy,\n    TrainingJobEarlyStoppingType,\n    HyperparameterTuningObjective,\n    HyperparameterScalingType,\n    ContinuousParameterRange,\n    IntegerParameterRange,\n    ParameterRangeOneOf,\n    AlgorithmSpecification,\n    InputContentType,\n    InputMode,\n    TrainingJobResourceConfig,\n    AlgorithmName,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the values of some hyperparameters, which will be used by the TrainingJob\nthese hyper-parameters are commonly used by the XGboost algorithm. Here we bootstrap them with some default Values\nUsually the default values are selected or \"tuned - refer to next section\"\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "xgboost_hyperparameters: typing.Dict[str, str] = {\n    \"num_round\": \"100\",\n    \"base_score\": \"0.5\",\n    \"booster\": \"gbtree\",\n    \"csv_weights\": \"0\",\n    \"dsplit\": \"row\",\n    \"grow_policy\": \"depthwise\",\n    \"lambda_bias\": \"0.0\",\n    \"max_bin\": \"256\",\n    \"normalize_type\": \"tree\",\n    \"objective\": \"reg:linear\",\n    \"one_drop\": \"0\",\n    \"prob_buffer_row\": \"1.0\",\n    \"process_type\": \"default\",\n    \"refresh_leaf\": \"1\",\n    \"sample_type\": \"uniform\",\n    \"scale_pos_weight\": \"1.0\",\n    \"silent\": \"0\",\n    \"skip_drop\": \"0.0\",\n    \"tree_method\": \"auto\",\n    \"tweedie_variance_power\": \"1.5\",\n    \"updater\": \"grow_colmaker,prune\",\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we define the actual algorithm (XGBOOST) and version of the algorithm to use\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alg_spec = AlgorithmSpecification(\n    input_mode=InputMode.FILE,\n    algorithm_name=AlgorithmName.XGBOOST,\n    algorithm_version=\"0.90\",\n    input_content_type=InputContentType.TEXT_CSV,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally lets use Flytekit plugin called SdkBuiltinAlgorithmTrainingJobTask, to create a task that wraps the algorithm.\nThis task does not really have a user-defined function as the actual algorithm is pre-defined in Sagemaker.\nBut, this task still has the same set of properties like any other FlyteTask\n- Caching\n- Resource specification\n- versioning etc\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "xgboost_train_task = SagemakerBuiltinAlgorithmsTask(\n    name=\"xgboost_trainer\",\n    task_config=SagemakerTrainingJobConfig(\n        algorithm_specification=alg_spec,\n        training_job_resource_config=TrainingJobResourceConfig(\n            instance_type=\"ml.m4.xlarge\", instance_count=1, volume_size_in_gb=25,\n        ),\n    ),\n    metadata=TaskMetadata(cache_version=\"1.0\", cache=True),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can use\n`Single task Execution <https://lyft.github.io/flyte/user/features/single_task_execution.html?highlight=single%20task%20execution>`_\nto execute just the task without needing to create a workflow.  To trigger an execution, you need to provide\n\nProject (flyteexamples) project where the execution will be created under\nDomain (development) domain where the execution will be created, under the project\ninputs - the actual inputs\n\nPre-built algorithms have restrictive set of inputs. They always expect\n\n#. Training data set\n#. Validation data set\n#. Static set of hyper parameters as a dictionary\n\nIn this case we have taken the `PIMA Diabetes dataset <https://www.kaggle.com/kumargh/pimaindiansdiabetescsv>`_\nand split it and uploaded to an s3 bucket\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def execute_training():\n    xgboost_train_task(\n        static_hyperparameters=xgboost_hyperparameters, train=\"\", validation=\"\",\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimize the hyper-parameters\nAmazon Sagemaker offers automatic hyper parameter blackbox optimization using HPO Service.\nThis technique is highly effective to figure out the right set of hyper parameters to use that\nimprove the overall accuracy of the model (or minimize the error)\nFlyte makes it extremely effective to optimize a model using Amazon Sagemaker HPO. In this example we will look how\nthis can be done for the prebuilt algorithm training done in the previous section\n\n## Define an HPO Task, that wraps the training task\nSo to start with hyper parameter optimization, once a training task is created, wrap it in\nSagemakerHPOTask as follows\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "xgboost_hpo_task = SagemakerHPOTask(\n    name=\"xgboost_hpo\",\n    task_config=HPOJob(\n        max_number_of_training_jobs=10,\n        max_parallel_training_jobs=5,\n        tunable_params=[\"num_round\", \"max_depth\", \"gamma\"],\n    ),\n    training_task=xgboost_train_task,\n    metadata=TaskMetadata(cache=True, cache_version=\"1.0\", retries=2),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch the HPO Job\nJust like Training Job, HPO job can be launched directly from the notebook The inputs for an HPO job that wraps a\ntraining job, are the combination of all inputs for the training job - i.e,\n\n#. \"train\" dataset, \"validation\" dataset and \"static hyper parameters\" for Training job\n#. HyperparameterTuningJobConfig, which consists of ParameterRanges, for the parameters that should be tuned,\n#. tuning strategy - Bayesian OR Random (or others as described in Sagemaker)\n#. Stopping condition and\n#. Objective metric name and type (whether to minimize etc)\n\nWhen launching the TrainingJob and HPOJob, we need to define the inputs.\nInputs are those directly related to algorithm outputs. We use the inputs\nand the version information to decide cache hit/miss\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def execute():\n    # TODO Local execution of hpo task is not supported. To add example of remote execution\n    xgboost_hpo_task(\n        # These 3 parameters are implicitly extracted from the training task itself\n        static_hyperparameters=xgboost_hyperparameters,\n        train=\"s3://demo/test-datasets/pima/train\",\n        validation=\"s3://demo/test-datasets/pima/validation\",\n        # The following parameteres are specific for hyper parameter tuning and allow to modify tuning at launch time\n        hyperparameter_tuning_job_config=HyperparameterTuningJobConfig(\n            tuning_strategy=HyperparameterTuningStrategy.BAYESIAN,\n            tuning_objective=HyperparameterTuningObjective(\n                objective_type=HyperparameterTuningObjectiveType.MINIMIZE,\n                metric_name=\"validation:error\",\n            ),\n            training_job_early_stopping_type=TrainingJobEarlyStoppingType.AUTO,\n        ),\n        # The following parameters are tunable parameters are specified during the configuration of the task\n        # this section provides the ranges to be sweeped\n        num_round=ParameterRangeOneOf(\n            param=IntegerParameterRange(\n                min_value=3, max_value=10, scaling_type=HyperparameterScalingType.LINEAR\n            )\n        ),\n        max_depth=ParameterRangeOneOf(\n            param=IntegerParameterRange(\n                min_value=5, max_value=7, scaling_type=HyperparameterScalingType.LINEAR\n            )\n        ),\n        gamma=ParameterRangeOneOf(\n            param=ContinuousParameterRange(\n                min_value=0.0,\n                max_value=0.3,\n                scaling_type=HyperparameterScalingType.LINEAR,\n            )\n        ),\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Register and launch the task standalone!\nhpo_exc = xgboost_hpo_task.register_and_launch(\"flyteexamples\", \"development\", inputs=hpo_inputs)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}