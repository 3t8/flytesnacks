{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# DataParallel distributed training of a Pytorch Model on Amazon Sagemaker using Flyte\n\nThis example is adapted from the following sagemake example.\nhttps://github.com/aws/amazon-sagemaker-examples/blob/89831fcf99ea3110f52794db0f6433a4013a5bca/sagemaker-python-sdk/pytorch_mnist/mnist.py\n\nIt shows how distributed training can be completely performed on the user side with minimal changes using Flyte.\n\nTODO: Flytekit will be adding further simplifications to make writing a distributed training algorithm even simpler, but\n      this example basically provides the full detailed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport os\nimport typing\nfrom dataclasses import dataclass\n\nimport flytekit\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.nn.functional as functional\nimport torch.optim as optim\nfrom dataclasses_json import dataclass_json\nfrom flytekit import task, workflow\nfrom flytekitplugins.awssagemaker import (\n    SagemakerTrainingJobConfig, AlgorithmSpecification, InputMode, AlgorithmName,\n    InputContentType, TrainingJobResourceConfig,\n)\nfrom flytekit.types.file import PythonPickledFile\nfrom torchvision import datasets, transforms\n\n\n@dataclass_json\n@dataclass\nclass Hyperparameters(object):\n    \"\"\"\n    Args:\n        batch_size: input batch size for training (default: 64)\n        test_batch_size: input batch size for testing (default: 1000)\n        epochs: number of epochs to train (default: 10)\n        learning_rate: learning rate (default: 0.01)\n        sgd_momentum: SGD momentum (default: 0.5)\n        seed: random seed (default: 1)\n        log_interval: how many batches to wait before logging training status\n        dir: directory where summary logs are stored\n    \"\"\"\n    backend: str = dist.Backend.GLOO\n    sgd_momentum: float = 0.5\n    seed: int = 1\n    log_interval: int = 10\n    batch_size: int = 64\n    test_batch_size: int = 1000\n    epochs: int = 10\n    learning_rate: float = 0.01\n\n\n@dataclass\nclass TrainingArgs(Hyperparameters):\n    \"\"\"\n    These are training arguments that contain additional metadata beyond the hyper parameters useful especially in\n    distributed training\n    \"\"\"\n    hosts: typing.List[int] = None\n    current_host: int = 0\n    num_gpus: int = 0\n    data_dir: str = \"/tmp\"\n    model_dir: str = \"/tmp\"\n\n    def is_distributed(self) -> bool:\n        return len(self.hosts) > 1 and self.backend is not None\n\n\n# Based on https://github.com/pytorch/examples/blob/master/mnist/main.py\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = functional.relu(functional.max_pool2d(self.conv1(x), 2))\n        x = functional.relu(functional.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = functional.relu(self.fc1(x))\n        x = functional.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return functional.log_softmax(x, dim=1)\n\n\ndef _get_train_data_loader(batch_size, training_dir, is_distributed, **kwargs):\n    logging.info(\"Get train data loader\")\n    dataset = datasets.MNIST(training_dir, train=True, download=False, transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ]))\n    logging.info(\"Dataset is downloaded. Creating a train_sampler\")\n    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset) if is_distributed else None\n    logging.info(\"Train_sampler is successfully created. Creating a DataLoader\")\n    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=train_sampler is None,\n                                       sampler=train_sampler, **kwargs)\n\n\ndef _get_test_data_loader(test_batch_size, training_dir, **kwargs):\n    logging.info(\"Get test data loader\")\n    return torch.utils.data.DataLoader(\n        datasets.MNIST(training_dir, train=False, download=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])),\n        batch_size=test_batch_size, shuffle=True, **kwargs)\n\n\ndef _average_gradients(model):\n    # Gradient averaging.\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n        param.grad.data /= size\n\n\ndef configure_model(model, is_distributed, gpu):\n    if is_distributed:\n        # multi-machine multi-gpu case\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu], output_device=gpu)\n    else:\n        # single-machine multi-gpu case or single-machine or multi-machine cpu case\n        model = torch.nn.DataParallel(model)\n    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Actual Trainer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train(gpu: int, args: TrainingArgs):\n    logging.basicConfig(level='INFO')\n    is_distributed = args.is_distributed()\n    logging.warning(\"Distributed training - {}\".format(is_distributed))\n    use_cuda = args.num_gpus > 0\n    logging.warning(\"Number of gpus available - {}\".format(args.num_gpus))\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    device = torch.device('cuda' if use_cuda else 'cpu')\n\n    rank = 0\n\n    if is_distributed:\n        # Initialize the distributed environment\n        world_size = len(args.hosts) * args.num_gpus\n        os.environ['WORLD_SIZE'] = str(world_size)\n        rank = args.hosts.index(args.current_host) * args.num_gpus + gpu\n        os.environ['RANK'] = str(rank)\n        dist.init_process_group(backend=args.backend, init_method='env://', rank=rank, world_size=world_size)\n        logging.info('Initialized the distributed environment: \\'{}\\' backend on {} nodes. '.format(\n            args.backend, dist.get_world_size()) + 'Current host rank is {}. Number of gpus: {}'.format(\n            dist.get_rank(), args.num_gpus))\n        torch.cuda.set_device(gpu)\n\n    # set the seed for generating random numbers\n    torch.manual_seed(args.seed)\n    if use_cuda:\n        torch.cuda.manual_seed(args.seed)\n\n    train_loader = _get_train_data_loader(args.batch_size, args.data_dir, is_distributed, **kwargs)\n    test_loader = _get_test_data_loader(args.test_batch_size, args.data_dir, **kwargs)\n\n    logging.info(\"Processes {}/{} ({:.0f}%) of train data\".format(\n        len(train_loader.sampler), len(train_loader.dataset),\n        100. * len(train_loader.sampler) / len(train_loader.dataset)\n    ))\n\n    logging.info(\"Processes {}/{} ({:.0f}%) of test data\".format(\n        len(test_loader.sampler), len(test_loader.dataset),\n        100. * len(test_loader.sampler) / len(test_loader.dataset)\n    ))\n\n    model = Net().to(device)\n\n    model = configure_model(model, is_distributed, gpu)\n\n    optimizer = optim.SGD(model.parameters(), lr=args.learning_rate, momentum=args.sgd_momentum)\n\n    logging.info(\"[rank {}|local-rank {}] Totally {} epochs\".format(rank, gpu, args.epochs + 1))\n    for epoch in range(1, args.epochs + 1):\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader, 1):\n            if use_cuda:\n                data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = functional.nll_loss(output, target)\n            loss.backward()\n            if is_distributed and not use_cuda:\n                # average gradients manually for multi-machine cpu case only\n                _average_gradients(model)\n            optimizer.step()\n            if batch_idx % args.log_interval == 0:\n                if not is_distributed or (is_distributed and rank == 0):\n                    logging.info('[rank {}|local-rank {}] Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f}'.format(\n                        rank, gpu,\n                        epoch, batch_idx * len(data), len(train_loader.sampler),\n                               100. * batch_idx / len(train_loader), loss.item()))\n        test(model, test_loader, device)\n\n    if not is_distributed or (is_distributed and rank == 0):\n        save_model(model, args.model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets test the trained model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader, device):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            if device.type == 'cuda':\n                data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n            output = model(data)\n            test_loss += functional.nll_loss(output, target, size_average=False).item()  # sum up batch loss\n            pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    logging.info('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\ndef model_fn(model_dir):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = torch.nn.DataParallel(Net())\n    with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:\n        model.load_state_dict(torch.load(f))\n    return model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the model to a local path\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def save_model(model, model_dir) -> PythonPickledFile:\n    logging.info(\"Saving the model.\")\n    path = os.path.join(model_dir, 'model.pth')\n    # recommended way from http://pytorch.org/docs/master/notes/serialization.html\n    torch.save(model.cpu().state_dict(), path)\n    print(f\"Model saved to {path}\")\n    return path\n\n\ndef download_training_data(training_dir):\n    logging.info(\"Downloading train data\")\n    datasets.MNIST(training_dir, train=True, download=True, transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ]))\n\n\ndef download_test_data(training_dir):\n    logging.info(\"Downloading test data\")\n    datasets.MNIST(training_dir, train=False, download=True, transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ]))\n\n\n# https://github.com/aws/amazon-sagemaker-examples/blob/89831fcf99ea3110f52794db0f6433a4013a5bca/sagemaker-python-sdk/pytorch_mnist/mnist.py\n@task(\n    task_config=SagemakerTrainingJobConfig(\n        algorithm_specification=AlgorithmSpecification(\n            input_mode=InputMode.FILE,\n            algorithm_name=AlgorithmName.CUSTOM,\n            algorithm_version=\"\",\n            input_content_type=InputContentType.TEXT_CSV,\n        ),\n        training_job_resource_config=TrainingJobResourceConfig(\n            instance_type=\"ml.p3.8xlarge\", instance_count=2, volume_size_in_gb=25,\n        ),\n    ),\n    cache_version=\"1.0\",\n    cache=True,\n    container_image=\"{{.image.sagemaker.fqn}}:smpytorch-{{.image.default.version}}\",\n)\ndef mnist_pytorch_job(hp: Hyperparameters) -> PythonPickledFile:\n    # pytorch's save() function does not create a path if the path specified does not exist\n    # therefore we must pass an existing path\n\n    ctx = flytekit.current_context()\n    data_dir = os.path.join(ctx.working_directory, \"data\")\n    model_dir = os.path.join(ctx.working_directory, \"model\")\n    os.makedirs(data_dir, exist_ok=True)\n    os.makedirs(model_dir, exist_ok=True)\n    args = TrainingArgs(\n        hosts=ctx.distributed_training_context.hosts,\n        current_host=ctx.distributed_training_context.current_host,\n        num_gpus=torch.cuda.device_count(),\n        batch_size=hp.batch_size,\n        test_batch_size=hp.test_batch_size,\n        epochs=hp.epochs,\n        learning_rate=hp.learning_rate,\n        sgd_momentum=hp.sgd_momentum,\n        seed=hp.seed,\n        log_interval=hp.log_interval,\n        backend=hp.backend,\n        data_dir=data_dir,\n        model_dir=model_dir,\n    )\n\n    # Data shouldn't be downloaded by the functions called in mp.spawn due to race conditions\n    # These can be replaced by Flyte's blob type inputs. Note that the data here are assumed\n    # to be accessible via a local path\n    download_training_data(args.data_dir)\n    download_test_data(args.data_dir)\n\n    if len(args.hosts) > 1:\n        # Config MASTER_ADDR and MASTER_PORT for PyTorch Distributed Training\n        os.environ['MASTER_ADDR'] = args.hosts[0]\n        os.environ['MASTER_PORT'] = '29500'\n        os.environ['NCCL_SOCKET_IFNAME'] = (ctx.distributed_training_context.network_interface_name)\n        os.environ['NCCL_DEBUG'] = 'INFO'\n        # The function is called as fn(i, *args), where i is the process index and args is the passed\n        # through tuple of arguments.\n        # https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn\n        mp.spawn(train, nprocs=args.num_gpus, args=(args,))\n    else:\n        # Config for Multi GPU with a single instance training\n        if args.num_gpus > 1:\n            gpu_devices = ','.join([str(gpu_id) for gpu_id in range(args.num_gpus)])\n            os.environ['CUDA_VISIBLE_DEVICES'] = gpu_devices\n        train(-1, args)\n\n    pth = os.path.join(model_dir, 'model.pth')\n    print(f\"Returning model @ {pth}\")\n    return pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a pipeline\nnow the training and the plotting can be together put into a pipeline, in which case the training is performed first\nfollowed by the plotting of the accuracy. Data is passed between them and the workflow itself outputs the image and\nthe serialize model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@workflow\ndef pytorch_training_wf(hp: Hyperparameters) -> PythonPickledFile:\n    return mnist_pytorch_job(hp=hp)\n\n\nif __name__ == \"__main__\":\n    model = pytorch_training_wf(hp=Hyperparameters(epochs=2, batch_size=128))\n    print(f\"Model: {model}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}