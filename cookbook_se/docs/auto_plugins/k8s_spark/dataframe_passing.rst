
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_plugins/k8s_spark/dataframe_passing.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_plugins_k8s_spark_dataframe_passing.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_plugins_k8s_spark_dataframe_passing.py:


.. _intermediate_spark_dataframes_passing:

Pass pandas dataframes
===========================

This example shows how users can return a spark.Dataset from a task and consume it as a pandas.DataFrame.
If the dataframe does not fit in memory, it will result in a runtime failure.

.. GENERATED FROM PYTHON SOURCE LINES 10-16

.. code-block:: default

    import flytekit
    import pandas
    from flytekit import task, workflow, kwtypes
    from flytekitplugins.spark import Spark
    from flytekit.types.schema import FlyteSchema


.. GENERATED FROM PYTHON SOURCE LINES 17-22

.. _df_my_schema_definition:

Define my_schema
--------------------
This section defines a simple schema type with 2 columns, `name: str` and `age: int`

.. GENERATED FROM PYTHON SOURCE LINES 22-25

.. code-block:: default

    my_schema = FlyteSchema[kwtypes(name=str, age=int)]



.. GENERATED FROM PYTHON SOURCE LINES 26-30

``create_spark_df`` is a spark task, that runs within a spark cotext (relies on having spark cluster up and running). This task generates a spark DataFrame whose schema matches the predefined :any:`df_my_schema_definition`
Notice that the task simply returns a pyspark.DataFrame object, even though the return type specifies  :any:`df_my_schema_definition`
The flytekit type-system will automatically convert the pyspark.DataFrame to Flyte Schema object.
FlyteSchema object is an abstract representation of a DataFrame, that can conform to multiple different dataframe formats.

.. GENERATED FROM PYTHON SOURCE LINES 30-54

.. code-block:: default

    @task(
        task_config=Spark(
            spark_conf={
                "spark.driver.memory": "1000M",
                "spark.executor.memory": "1000M",
                "spark.executor.cores": "1",
                "spark.executor.instances": "2",
                "spark.driver.cores": "1",
            }
        ),
        cache_version="1",
        container_image="{{.image.default.fqn}}:spark-{{.image.default.version}}",
    )
    def create_spark_df() -> my_schema:
        """
        This spark program returns a spark dataset that conforms to the defined schema. Failure to do so should result
        in a runtime error. TODO: runtime error enforcement
        """
        sess = flytekit.current_context().spark_session
        return sess.createDataFrame(
            [("Alice", 5), ("Bob", 10), ("Charlie", 15),], my_schema.column_names(),
        )



.. GENERATED FROM PYTHON SOURCE LINES 55-60

The task ``sum_of_all_ages`` receives a parameter of type :any:`df_my_schema_definition`. It is important to note that there is no
expectation that the schema is a pandas dataframe or a spark dataframe, but just a generic schema object. The Flytekit schema object
can be read into multiple formats using the ``open()`` method. Default conversion is to :py:class:`pandas.DataFrame`
Refer to :py:class:`flytekit.FlyteSchema` for more details


.. GENERATED FROM PYTHON SOURCE LINES 60-74

.. code-block:: default

    @task(cache_version="1")
    def sum_of_all_ages(s: my_schema) -> int:
        """
        The schema is passed into this task. Schema is just a reference to the actually object and has almost no overhead.
        Only performing an ``open`` on the schema will cause the data to be loaded into memory (also downloaded if this being
        run in a remote setting)
        """
        # This by default returns a pandas.DataFrame object. ``open`` can be parameterized to return other dataframe types
        reader = s.open()
        # supported dataframes
        df: pandas.DataFrame = reader.all()
        return df["age"].sum()



.. GENERATED FROM PYTHON SOURCE LINES 75-76

The schema workflow allows connecting the ``create_spark_df`` with  ``sum_of_all_ages`` because the return type of the first task and the parameter type for the second task match

.. GENERATED FROM PYTHON SOURCE LINES 76-86

.. code-block:: default

    @workflow
    def my_smart_schema() -> int:
        """
        This workflow shows how a simple schema can be created in spark and passed to a python function and accessed as a
        pandas.DataFrame. Flyte Schemas are abstract data frames and not really tied to a specific memory representation.
        """
        df = create_spark_df()
        return sum_of_all_ages(s=df)



.. GENERATED FROM PYTHON SOURCE LINES 87-89

This program can be executed locally and it should work as expected and this greatly simplifies usage of disparate DataFrame technologies for the end user.
Also new DataFrame technologies can be dynamically loaded in flytekit's TypeEngine. More on this later.

.. GENERATED FROM PYTHON SOURCE LINES 89-95

.. code-block:: default

    if __name__ == "__main__":
        """
        This program can be run locally
        """
        print(f"Running {__file__} main...")
        print(f"Running my_smart_schema()-> {my_smart_schema()}")


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.000 seconds)


.. _sphx_glr_download_auto_plugins_k8s_spark_dataframe_passing.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: dataframe_passing.py <dataframe_passing.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: dataframe_passing.ipynb <dataframe_passing.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
