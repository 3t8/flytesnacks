{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Pass pandas dataframes\n\nThis example shows how users can return a spark.Dataset from a task and consume it as a pandas.DataFrame.\nIf the dataframe does not fit in memory, it will result in a runtime failure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import flytekit\nimport pandas\nfrom flytekit import task, workflow, kwtypes\nfrom flytekitplugins.spark import Spark\nfrom flytekit.types.schema import FlyteSchema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Define my_schema\nThis section defines a simple schema type with 2 columns, `name: str` and `age: int`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "my_schema = FlyteSchema[kwtypes(name=str, age=int)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``create_spark_df`` is a spark task, that runs within a spark cotext (relies on having spark cluster up and running). This task generates a spark DataFrame whose schema matches the predefined :any:`df_my_schema_definition`\nNotice that the task simply returns a pyspark.DataFrame object, even though the return type specifies  :any:`df_my_schema_definition`\nThe flytekit type-system will automatically convert the pyspark.DataFrame to Flyte Schema object.\nFlyteSchema object is an abstract representation of a DataFrame, that can conform to multiple different dataframe formats.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@task(\n    task_config=Spark(\n        spark_conf={\n            \"spark.driver.memory\": \"1000M\",\n            \"spark.executor.memory\": \"1000M\",\n            \"spark.executor.cores\": \"1\",\n            \"spark.executor.instances\": \"2\",\n            \"spark.driver.cores\": \"1\",\n        }\n    ),\n    cache_version=\"1\",\n    container_image=\"{{.image.default.fqn}}:spark-{{.image.default.version}}\",\n)\ndef create_spark_df() -> my_schema:\n    \"\"\"\n    This spark program returns a spark dataset that conforms to the defined schema. Failure to do so should result\n    in a runtime error. TODO: runtime error enforcement\n    \"\"\"\n    sess = flytekit.current_context().spark_session\n    return sess.createDataFrame(\n        [(\"Alice\", 5), (\"Bob\", 10), (\"Charlie\", 15),], my_schema.column_names(),\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The task ``sum_of_all_ages`` receives a parameter of type :any:`df_my_schema_definition`. It is important to note that there is no\nexpectation that the schema is a pandas dataframe or a spark dataframe, but just a generic schema object. The Flytekit schema object\ncan be read into multiple formats using the ``open()`` method. Default conversion is to :py:class:`pandas.DataFrame`\nRefer to :py:class:`flytekit.FlyteSchema` for more details\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@task(cache_version=\"1\")\ndef sum_of_all_ages(s: my_schema) -> int:\n    \"\"\"\n    The schema is passed into this task. Schema is just a reference to the actually object and has almost no overhead.\n    Only performing an ``open`` on the schema will cause the data to be loaded into memory (also downloaded if this being\n    run in a remote setting)\n    \"\"\"\n    # This by default returns a pandas.DataFrame object. ``open`` can be parameterized to return other dataframe types\n    reader = s.open()\n    # supported dataframes\n    df: pandas.DataFrame = reader.all()\n    return df[\"age\"].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The schema workflow allows connecting the ``create_spark_df`` with  ``sum_of_all_ages`` because the return type of the first task and the parameter type for the second task match\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@workflow\ndef my_smart_schema() -> int:\n    \"\"\"\n    This workflow shows how a simple schema can be created in spark and passed to a python function and accessed as a\n    pandas.DataFrame. Flyte Schemas are abstract data frames and not really tied to a specific memory representation.\n    \"\"\"\n    df = create_spark_df()\n    return sum_of_all_ages(s=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This program can be executed locally and it should work as expected and this greatly simplifies usage of disparate DataFrame technologies for the end user.\nAlso new DataFrame technologies can be dynamically loaded in flytekit's TypeEngine. More on this later.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    \"\"\"\n    This program can be run locally\n    \"\"\"\n    print(f\"Running {__file__} main...\")\n    print(f\"Running my_smart_schema()-> {my_smart_schema()}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}