
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_plugins/k8s_spark/pyspark_pi.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_plugins_k8s_spark_pyspark_pi.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_plugins_k8s_spark_pyspark_pi.py:


.. _intermediate_using_spark_tasks:

Creating spark tasks as part of your workflow OR running spark jobs
------------------------------------------------------------------------

This example shows how flytekit simplifies usage of pyspark in a users code.
The task ``hello_spark`` runs a new spark cluster, which when run locally runs a single node client only cluster,
but when run remote spins up a arbitrarily sized cluster depending on the specified spark configuration. ``spark_conf``

This Example also shows how a user can simply create 2 tasks, that use different Docker images. For more information refer to :any:`hosted_multi_images`

.. GENERATED FROM PYTHON SOURCE LINES 14-20

.. code-block:: default

    import flytekit
    import random
    import datetime
    from operator import add
    from flytekit import task, workflow


.. GENERATED FROM PYTHON SOURCE LINES 21-22

The follow import is required to configure a Spark Server in Flyte.

.. GENERATED FROM PYTHON SOURCE LINES 22-25

.. code-block:: default

    from flytekitplugins.spark import Spark



.. GENERATED FROM PYTHON SOURCE LINES 26-30

Spark Task sample. This example shows how a spark task can be written simply by adding a ``@task(task_config=Spark(...)...)`` decorator.
Refer to :py:class:`flytekit.Spark` class to understand the various configuration options.
Also important to note here that the container_image is a special image that is built as part of the samples repo. To understand how to configure
different containers per task refer to :any:`hosted_multi_images`.

.. GENERATED FROM PYTHON SOURCE LINES 30-64

.. code-block:: default

    @task(
        task_config=Spark(
            # this configuration is applied to the spark cluster
            spark_conf={
                "spark.driver.memory": "1000M",
                "spark.executor.memory": "1000M",
                "spark.executor.cores": "1",
                "spark.executor.instances": "2",
                "spark.driver.cores": "1",
            }
        ),
        cache_version="1",
        # a separate image is used
        container_image="{{.image.default.fqn}}:spark-{{.image.default.version}}",
    )
    def hello_spark(partitions: int) -> float:
        print("Starting Spark with Partitions: {}".format(partitions))

        n = 100000 * partitions
        sess = flytekit.current_context().spark_session
        count = (
            sess.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)
        )
        pi_val = 4.0 * count / n
        print("Pi val is :{}".format(pi_val))
        return pi_val


    def f(_):
        x = random.random() * 2 - 1
        y = random.random() * 2 - 1
        return 1 if x ** 2 + y ** 2 <= 1 else 0



.. GENERATED FROM PYTHON SOURCE LINES 65-66

This is a regular python function task. This will not execute on the spark cluster

.. GENERATED FROM PYTHON SOURCE LINES 66-72

.. code-block:: default

    @task(cache_version="1")
    def print_every_time(value_to_print: float, date_triggered: datetime.datetime) -> int:
        print("My printed value: {} @ {}".format(value_to_print, date_triggered))
        return 1



.. GENERATED FROM PYTHON SOURCE LINES 73-74

The Workflow shows that a spark task and any python function (or any other task type) can be chained together as long as they match the parameter specifications

.. GENERATED FROM PYTHON SOURCE LINES 74-85

.. code-block:: default

    @workflow
    def my_spark(triggered_date: datetime.datetime) -> float:
        """
        Using the workflow is still as any other workflow. As image is a property of the task, the workflow does not care
        about how the image is configured.
        """
        pi = hello_spark(partitions=50)
        print_every_time(value_to_print=pi, date_triggered=triggered_date)
        return pi



.. GENERATED FROM PYTHON SOURCE LINES 86-87

Workflows with spark tasks can be executed locally. Some aspects of spark, like links to plugins_hive metastores etc may not work, but these are limitations of using Spark and are not introduced by Flyte.

.. GENERATED FROM PYTHON SOURCE LINES 87-96

.. code-block:: default

    if __name__ == "__main__":
        """
        NOTE: To run a multi-image workflow locally, all dependencies of all the tasks should be installed, ignoring which
        may result in local runtime failures.
        """
        print(f"Running {__file__} main...")
        print(
            f"Running my_spark(triggered_date=datetime.datetime.now()){my_spark(triggered_date=datetime.datetime.now())}"
        )


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.000 seconds)


.. _sphx_glr_download_auto_plugins_k8s_spark_pyspark_pi.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: pyspark_pi.py <pyspark_pi.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: pyspark_pi.ipynb <pyspark_pi.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
