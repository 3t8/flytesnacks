{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Creating spark tasks as part of your workflow OR running spark jobs\n\nThis example shows how flytekit simplifies usage of pyspark in a users code.\nThe task ``hello_spark`` runs a new spark cluster, which when run locally runs a single node client only cluster,\nbut when run remote spins up a arbitrarily sized cluster depending on the specified spark configuration. ``spark_conf``\n\nThis Example also shows how a user can simply create 2 tasks, that use different Docker images. For more information refer to :any:`hosted_multi_images`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import flytekit\nimport random\nimport datetime\nfrom operator import add\nfrom flytekit import task, workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The follow import is required to configure a Spark Server in Flyte.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from flytekitplugins.spark import Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Spark Task sample. This example shows how a spark task can be written simply by adding a ``@task(task_config=Spark(...)...)`` decorator.\nRefer to :py:class:`flytekit.Spark` class to understand the various configuration options.\nAlso important to note here that the container_image is a special image that is built as part of the samples repo. To understand how to configure\ndifferent containers per task refer to :any:`hosted_multi_images`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@task(\n    task_config=Spark(\n        # this configuration is applied to the spark cluster\n        spark_conf={\n            \"spark.driver.memory\": \"1000M\",\n            \"spark.executor.memory\": \"1000M\",\n            \"spark.executor.cores\": \"1\",\n            \"spark.executor.instances\": \"2\",\n            \"spark.driver.cores\": \"1\",\n        }\n    ),\n    cache_version=\"1\",\n    # a separate image is used\n    container_image=\"{{.image.default.fqn}}:spark-{{.image.default.version}}\",\n)\ndef hello_spark(partitions: int) -> float:\n    print(\"Starting Spark with Partitions: {}\".format(partitions))\n\n    n = 100000 * partitions\n    sess = flytekit.current_context().spark_session\n    count = (\n        sess.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n    )\n    pi_val = 4.0 * count / n\n    print(\"Pi val is :{}\".format(pi_val))\n    return pi_val\n\n\ndef f(_):\n    x = random.random() * 2 - 1\n    y = random.random() * 2 - 1\n    return 1 if x ** 2 + y ** 2 <= 1 else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a regular python function task. This will not execute on the spark cluster\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@task(cache_version=\"1\")\ndef print_every_time(value_to_print: float, date_triggered: datetime.datetime) -> int:\n    print(\"My printed value: {} @ {}\".format(value_to_print, date_triggered))\n    return 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Workflow shows that a spark task and any python function (or any other task type) can be chained together as long as they match the parameter specifications\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@workflow\ndef my_spark(triggered_date: datetime.datetime) -> float:\n    \"\"\"\n    Using the workflow is still as any other workflow. As image is a property of the task, the workflow does not care\n    about how the image is configured.\n    \"\"\"\n    pi = hello_spark(partitions=50)\n    print_every_time(value_to_print=pi, date_triggered=triggered_date)\n    return pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Workflows with spark tasks can be executed locally. Some aspects of spark, like links to plugins_hive metastores etc may not work, but these are limitations of using Spark and are not introduced by Flyte.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    \"\"\"\n    NOTE: To run a multi-image workflow locally, all dependencies of all the tasks should be installed, ignoring which\n    may result in local runtime failures.\n    \"\"\"\n    print(f\"Running {__file__} main...\")\n    print(\n        f\"Running my_spark(triggered_date=datetime.datetime.now()){my_spark(triggered_date=datetime.datetime.now())}\"\n    )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}